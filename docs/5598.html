<html>
<head>
<title>Machine Learning Theory and Programming — Unsupervised Learning: K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习理论与程序设计——无监督学习:K-均值聚类</h1>
<blockquote>原文：<a href="https://javascript.plainenglish.io/machine-learning-theory-and-programming-unsupervised-learning-k-means-clustering-52eeea41cba0?source=collection_archive---------10-----------------------#2021-11-22">https://javascript.plainenglish.io/machine-learning-theory-and-programming-unsupervised-learning-k-means-clustering-52eeea41cba0?source=collection_archive---------10-----------------------#2021-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6b83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一种流行的机器学习算法介绍</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/ac0e05fe14967e1e589aa7ab41614bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avIUzkZZcL_Ix10zGYxjbg.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image credit: Jon Radchenko</figcaption></figure><p id="d6c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们走过了很多有监督的<a class="ae lb" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法、<a class="ae lb" href="https://betterprogramming.pub/machine-learning-theory-and-programming-supervised-learning-regression-analysis-8ed2d86f5714" rel="noopener ugc nofollow" target="_blank">线性回归、多项式回归</a>、<a class="ae lb" href="https://enlear.academy/logistic-regression-in-machine-learning-672c0e8c8053" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>、<a class="ae lb" href="https://medium.com/geekculture/machine-learning-theory-and-programming-supervised-learning-neural-networks-74a598cb9e42" rel="noopener">神经网络</a>、<a class="ae lb" rel="noopener ugc nofollow" target="_blank" href="/machine-learning-theory-and-programming-supervised-learning-support-vector-machine-d6cc7a5747f1">支持向量机</a> (SVMs)、以及<a class="ae lb" rel="noopener ugc nofollow" target="_blank" href="/machine-learning-theory-and-programming-supervised-learning-for-multiclass-classification-ee0d9d32150e">多分类器</a>。监督学习建立了一组数据的数学模型，其中包含输入(<code class="fe lc ld le lf b">x</code>)和正确的输出(<code class="fe lc ld le lf b">y</code>)。</p><p id="b942" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank">无监督学习</a>是一种机器学习，其中算法没有为训练数据提供任何正确的输出(<code class="fe lc ld le lf b">y</code>)。它发现训练数据集中自然出现的模式。聚类是一种无监督的学习算法，它将未标记的数据集(没有正确的输出)分组到不同的簇中。K均值聚类算法将训练数据分组为K个聚类。它是最简单和最流行的无监督机器学习算法之一。</p><p id="e6d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我们来看看K-means聚类是如何工作的。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><h1 id="eaaa" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">k-均值聚类理论</h1><p id="4ce2" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">我们假设数据集中有K个聚类。K-means算法随机初始化K个聚类质心，<code class="fe lc ld le lf b">μ₁</code>，<code class="fe lc ld le lf b">μ₂</code>，…，<code class="fe lc ld le lf b">μₖ</code>。</p><p id="af85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">重复以下循环，直到质心稳定(收敛)或达到预定义的迭代次数:</p><ul class=""><li id="6878" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated"><code class="fe lc ld le lf b">for i = 1 to m</code>，找出哪个质心最接近<code class="fe lc ld le lf b">x⁽ⁱ⁾</code>。将簇索引分配给<code class="fe lc ld le lf b">c⁽ⁱ⁾</code>。</li><li id="3710" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated"><code class="fe lc ld le lf b">for k = 1 to K</code>，计算分配给<code class="fe lc ld le lf b">kth</code>质心的点的平均值。将平均值分配给<code class="fe lc ld le lf b">μₖ</code>作为新的质心。</li></ul><p id="2a73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该算法的目标是找到聚类，使得总成本<code class="fe lc ld le lf b">J(c⁽¹⁾, c⁽²⁾,... ,c⁽ᵐ⁾,μ₁, μ₂, …, μₖ)</code>最低。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ne"><img src="../Images/a61c4e6a0d467fa44014ae70fdfc374f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nokFZtBwMvukl4g3zshwQw.png"/></div></div></figure><p id="7c00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe lc ld le lf b">||x — μ||²</code>是两点间欧几里得距离的平方。</p><p id="bdee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是从<a class="ae lb" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank">维基百科</a>下载的动态gif图片。它说明了三个质心如何从随机位置开始并收敛到三个聚类中心。在此过程中，数据集被分为三个集群。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/1acc2d9d427aa6533761ff49c967e029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/1*pwCAVEzTCi22Ihto234ZlA.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Chire, CC BY-SA 4.0 &lt;<a class="ae lb" href="https://creativecommons.org/licenses/by-sa/4.0" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/licenses/by-sa/4.0</a>&gt;, via Wikimedia Commons</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><h1 id="fd68" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">k-均值聚类编程</h1><p id="2d41" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">还有机器学习专用的编程语言，比如<a class="ae lb" href="https://matlab.mathworks.com/" rel="noopener ugc nofollow" target="_blank"> MATLAB </a>、<a class="ae lb" href="https://www.gnu.org/software/octave/index" rel="noopener ugc nofollow" target="_blank"> Octave </a>、<a class="ae lb" href="https://betterprogramming.pub/exploring-the-ai-programming-language-r-102d25af9646" rel="noopener ugc nofollow" target="_blank"> R </a>等。对于一些通用编程语言，如Python，它们提供了机器学习库。强烈建议不要多此一举。我们可以使用机器学习特定的编程语言或机器学习库来解决多类分类问题。</p><p id="bfed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">MATLAB是MathWorks开发的专有多范式编程语言和数值计算环境。它提供了一个内置函数<code class="fe lc ld le lf b"><a class="ae lb" href="https://www.mathworks.com/help/stats/kmeans.html?searchHighlight=kmeans&amp;s_tid=srchtitle" rel="noopener ugc nofollow" target="_blank">kmeans</a>(X,k)</code>，用于执行K-means聚类，将矩阵<code class="fe lc ld le lf b">X</code>中的观察值划分为<code class="fe lc ld le lf b">k</code>个聚类。它返回一个向量(<code class="fe lc ld le lf b">idx</code>)，包含每个观察值的聚类索引。</p><h2 id="bddc" class="ng lo iq bd lp nh ni dn lt nj nk dp lx jy nl nm mb kc nn no mf kg np nq mj nr bi translated">虹膜数据的k-均值聚类</h2><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ns"><img src="../Images/95a368abe7be47634442eae96768d521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6kpyYH-GZJPAVs4USJgBA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image credit: Wikipedia</figcaption></figure><p id="496c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank">费希尔的虹膜数据</a>是模式识别文献中最著名的数据库。它包括对150种鸢尾属植物的萼片长度、萼片宽度、花瓣长度和花瓣宽度的测量(在<code class="fe lc ld le lf b">cm</code>中)。<code class="fe lc ld le lf b">setosa</code>、<code class="fe lc ld le lf b">versicolor</code>和<code class="fe lc ld le lf b">virginica</code>三个物种各有50个标本。我们关注花瓣长度和花瓣宽度，它们是<code class="fe lc ld le lf b">meas</code>矩阵中的第3列和第4列。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/240c3deafddde3c6e96bb5d83544d591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*cji8UvB8hGzMZYL5cWEmDw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image by author</figcaption></figure><p id="8ceb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经使用监督学习来分类iris类。这里我们使用无监督的K-means对它们进行分类。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="060f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第1行指定了函数名<code class="fe lc ld le lf b">kMeansClustering</code>。它有一个用于目标集群计数的参数<code class="fe lc ld le lf b">k</code>。</p><p id="9e1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第3–5行加载数据集<code class="fe lc ld le lf b">fisheriris</code>，并将数据分配给<code class="fe lc ld le lf b">X</code>和<code class="fe lc ld le lf b">y</code>。<code class="fe lc ld le lf b">X</code>取虹膜数据的第3列和第4列，分别是花瓣长度和花瓣宽度。<code class="fe lc ld le lf b">y</code>是从<code class="fe lc ld le lf b">species</code>复制的单元格数组。</p><p id="644e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第8行指定随机数生成器的种子以控制再现性。</p><p id="547e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第11–12行使用内置函数<code class="fe lc ld le lf b">kmeans</code>来训练模型。第11行设置迭代输出的配置。第12行使用迭代输出运行<code class="fe lc ld le lf b">kmeans</code>，并返回经过训练的质心<code class="fe lc ld le lf b">C</code>。</p><p id="1135" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第15-18行生成二维网格上的每个点，<code class="fe lc ld le lf b">xGrid</code>。密度由<code class="fe lc ld le lf b">d</code>(第15行)定义。<code class="fe lc ld le lf b">meshgrid(x, y)</code>根据矢量<code class="fe lc ld le lf b">x</code>和<code class="fe lc ld le lf b">y</code>中包含的坐标返回二维网格坐标。</p><p id="8576" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第21-23行用一次<code class="fe lc ld le lf b">kmeans</code>迭代为训练过的<code class="fe lc ld le lf b">C</code>检查每个二维网格坐标。结果指标存储在<code class="fe lc ld le lf b">idxRegion</code>中，由<code class="fe lc ld le lf b">gscatter</code>绘制，其中<code class="fe lc ld le lf b">Region 1</code>用洋红色(<code class="fe lc ld le lf b">'m'</code>)<code class="fe lc ld le lf b">Region 2</code>用黄色(<code class="fe lc ld le lf b">'y'</code>)<code class="fe lc ld le lf b">Region 3</code>用青色(<code class="fe lc ld le lf b">'c'</code>)。第23行调用<code class="fe lc ld le lf b">hold on</code>来保留当前轴中的图，以便添加到轴中的新图不会删除现有的图。</p><p id="062e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第26行在彩色区域的顶部绘制了虹膜训练集<code class="fe lc ld le lf b">X</code>。</p><p id="b689" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第29行在现有图形的顶部绘制了经过训练的质心<code class="fe lc ld le lf b">C</code>，其中<code class="fe lc ld le lf b">marker</code>是一个十字(<code class="fe lc ld le lf b">x</code>)，带有黑色的<code class="fe lc ld le lf b">color</code>(<code class="fe lc ld le lf b">k</code>)。<code class="fe lc ld le lf b">MarkerSize</code>是15，<code class="fe lc ld le lf b">linewidth</code>是3。</p><p id="9366" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第32–37行绘制标题、轴标签和图例。</p><p id="017a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第38行设置了数据范围的轴限制。</p><p id="1d6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第39行调用<code class="fe lc ld le lf b">hold off</code>将保持状态设置为关闭。</p><p id="46b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第40行终止该函数。</p><p id="665f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在MATLAB命令窗口上运行程序:</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="6199" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于我们设置了<code class="fe lc ld le lf b">statset('Display', 'iter')</code>，所以显示的是迭代输出。需要4次迭代才能收敛。最佳距离总和<code class="fe lc ld le lf b">sumd</code>为<code class="fe lc ld le lf b">31.4129</code>。</p><p id="f19d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是<code class="fe lc ld le lf b">kMeansClustering(3)</code>的分类:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c278eb284bafd315a2c48c3d96373360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*DuKAlGwTvEq0fXD8XMtTWQ.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image by author</figcaption></figure><h2 id="8ab2" class="ng lo iq bd lp nh ni dn lt nj nk dp lx jy nl nm mb kc nn no mf kg np nq mj nr bi translated">确定聚类数</h2><p id="8628" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">我们已经使用K-均值聚类来分类三个虹膜类。如果我们不知道要分类的聚类数，那该怎么办？</p><p id="595f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以运行<code class="fe lc ld le lf b">kMeansClustering(2)</code>，得到两个类的分类:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/abfea5978ddfd5e315d3b000b33471f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*tHQ9KespgLdoEJNSkYG7ow.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image by author</figcaption></figure><p id="fdd2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也可以运行<code class="fe lc ld le lf b">kMeansClustering(4)</code>，得到四个类的分类:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bad5cf5f64bf7e7d6b54afeca56384c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*G3VXMLG3c5dXxJ_HF1fvsA.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image by author</figcaption></figure><p id="d8a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">两节课和四节课看起来都是合理的答案。</p><p id="5c1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘方法</a>是确定数据集中聚类数量的启发式方法。在数学优化中，使用“肘”作为分界点是一种常见的启发式方法，以选择一个收益递减不再值得额外成本的点。</p><p id="519f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们运行下面的算法来显示聚类数和成本函数j之间的关系。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="2a33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第1行指定了函数名<code class="fe lc ld le lf b">kMeansElbowGraph</code>。</p><p id="9959" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第3–4行加载数据集<code class="fe lc ld le lf b">fisheriris</code>，并将数据分配给<code class="fe lc ld le lf b">X</code>。<code class="fe lc ld le lf b">X</code>获取第3列和第4列虹膜数据，即花瓣长度和花瓣宽度。</p><p id="496b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第7行指定了随机数生成器的种子以控制再现性。</p><p id="b94a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第10–16行训练K-means模型，使聚类数为1，2，…，10。训练值<code class="fe lc ld le lf b">sumd</code>被汇总并与k一起存储</p><p id="756c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第19行描绘了<code class="fe lc ld le lf b">k</code>和<code class="fe lc ld le lf b">sumd</code>之间的关系。线条颜色为蓝色(<code class="fe lc ld le lf b">b</code>)，而<code class="fe lc ld le lf b">linewidth</code>为2。</p><p id="aa16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第22–24行绘制标题和轴标签。</p><p id="fb0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第27行设置x轴的范围。</p><p id="8c7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第28行结束了这个函数。</p><p id="b7ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行程序，我们得到了肘图:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9fd941e561dc6912ce88854bad29edca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*A6Kl-RGvi_5mgyka9JRnIg.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Image by author</figcaption></figure><p id="fbc4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图清楚地显示了<code class="fe lc ld le lf b">k = 2</code>或<code class="fe lc ld le lf b">k = 4</code>是可接受的选择，但<code class="fe lc ld le lf b">k = 3</code>是最佳选择。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><h1 id="74f1" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">结论</h1><p id="873d" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">有很多机器学习算法。在这篇文章中，我们提出了K-means聚类的无监督学习。机器学习编程语言设计有预建的库和对数据科学和数据模型的高级支持。MATLAB的内置函数<code class="fe lc ld le lf b">kmeans</code>，让训练变得简单有效。</p><p id="9ba0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是其他机器学习算法的列表:</p><ul class=""><li id="1781" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated"><a class="ae lb" href="https://betterprogramming.pub/machine-learning-theory-and-programming-supervised-learning-regression-analysis-8ed2d86f5714" rel="noopener ugc nofollow" target="_blank">回归分析</a></li><li id="9b42" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated"><a class="ae lb" href="https://enlear.academy/logistic-regression-in-machine-learning-672c0e8c8053" rel="noopener ugc nofollow" target="_blank">逻辑回归</a></li><li id="25e7" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated"><a class="ae lb" href="https://medium.com/geekculture/machine-learning-theory-and-programming-supervised-learning-neural-networks-74a598cb9e42" rel="noopener">神经网络</a></li><li id="12bb" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated"><a class="ae lb" rel="noopener ugc nofollow" target="_blank" href="/machine-learning-theory-and-programming-supervised-learning-support-vector-machine-d6cc7a5747f1">支持向量机</a></li><li id="a9a4" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated"><a class="ae lb" rel="noopener ugc nofollow" target="_blank" href="/machine-learning-theory-and-programming-supervised-learning-for-multiclass-classification-ee0d9d32150e">多重量词</a></li></ul><p id="d3a4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读。我希望这有所帮助。如果你有兴趣，可以看看<a class="ae lb" href="https://jenniferfubook.medium.com/jennifer-fus-web-development-publications-1a887e4454af" rel="noopener">我的其他媒体文章</a>。</p><p id="ae45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">注:感谢Josh Poduska、Andrew Ziegler和Subir Mansukhani推荐机器学习资源！还有，感谢吴恩达教授的</em> <a class="ae lb" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="nx">机器学习课</em> </a> <em class="nx">。</em></p><p id="8be6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">更多内容请看</em><a class="ae lb" href="http://plainenglish.io/" rel="noopener ugc nofollow" target="_blank"><em class="nx">plain English . io</em></a><em class="nx">。在这里报名参加我们的</em> <a class="ae lb" href="http://newsletter.plainenglish.io/" rel="noopener ugc nofollow" target="_blank"> <em class="nx">免费周报</em> </a> <em class="nx">。</em></p></div></div>    
</body>
</html>